# Databricks notebook source
# MAGIC %md
# MAGIC # Pneumonia Classification POC - Deploy Serving Endpoint
# MAGIC
# MAGIC This notebook deploys the trained model as a REST API serving endpoint:
# MAGIC 1. Deploy model from MLflow Model Registry as serving endpoint
# MAGIC 2. Wait for endpoint to be READY
# MAGIC 3. Test endpoint with sample predictions
# MAGIC
# MAGIC **Prerequisites**: Run `train_poc_model.py` first to register model in MLflow
# MAGIC
# MAGIC **Output**: Live REST API endpoint for real-time predictions

# COMMAND ----------
# MAGIC %md
# MAGIC ## Step 1: Configuration and Imports

# COMMAND ----------
import requests
import json
import time
from PIL import Image
import numpy as np

# Configuration
MODEL_NAME = "healthcare_catalog_dev.models.pneumonia_poc_classifier"  # Unity Catalog full path
MODEL_VERSION = "1"  # Use version 1 (latest)
ENDPOINT_NAME = "pneumonia-poc-classifier"
WORKLOAD_SIZE = "Small"  # Small serverless endpoint (cost-effective)

# Get workspace details
workspace_url = spark.conf.get("spark.databricks.workspaceUrl")
token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()

print(f"Configuration:")
print(f"  Workspace URL: {workspace_url}")
print(f"  Model Name: {MODEL_NAME}")
print(f"  Model Version: {MODEL_VERSION}")
print(f"  Endpoint Name: {ENDPOINT_NAME}")

# COMMAND ----------
# MAGIC %md
# MAGIC ## Step 2: Deploy Model Serving Endpoint
# MAGIC
# MAGIC This creates a serverless REST API endpoint for the model

# COMMAND ----------
# Check if endpoint already exists
endpoint_list_url = f"https://{workspace_url}/api/2.0/serving-endpoints"

response = requests.get(
    endpoint_list_url,
    headers={"Authorization": f"Bearer {token}"}
)

existing_endpoints = response.json().get("endpoints", [])
endpoint_exists = any(ep["name"] == ENDPOINT_NAME for ep in existing_endpoints)

if endpoint_exists:
    print(f"‚ö†Ô∏è  Endpoint '{ENDPOINT_NAME}' already exists")
    print(f"Checking status...")

    endpoint_url = f"https://{workspace_url}/api/2.0/serving-endpoints/{ENDPOINT_NAME}"
    response = requests.get(
        endpoint_url,
        headers={"Authorization": f"Bearer {token}"}
    )

    endpoint_info = response.json()
    status = endpoint_info.get("state", {}).get("ready", "UNKNOWN")

    print(f"Current status: {status}")

    if status != "READY":
        print(f"\nEndpoint exists but not READY. Please wait or delete and recreate.")
    else:
        print(f"\n‚úÖ Endpoint is READY! Skip to Step 4 to test it.")
else:
    print(f"Creating new serving endpoint: {ENDPOINT_NAME}")

    endpoint_config = {
        "name": ENDPOINT_NAME,
        "config": {
            "served_models": [{
                "model_name": MODEL_NAME,
                "model_version": MODEL_VERSION,
                "workload_size": WORKLOAD_SIZE,
                "scale_to_zero_enabled": True
            }]
        }
    }

    response = requests.post(
        endpoint_list_url,
        headers={"Authorization": f"Bearer {token}"},
        json=endpoint_config
    )

    if response.status_code in [200, 201]:
        print(f"‚úÖ Endpoint deployment started!")
        print(f"\nResponse: {json.dumps(response.json(), indent=2)}")
        print(f"\nEndpoint will be ready in ~5-10 minutes...")
        print(f"Continue to Step 3 to monitor deployment status.")
    else:
        print(f"‚ùå Error creating endpoint: {response.status_code}")
        print(f"Response: {response.text}")

# COMMAND ----------
# MAGIC %md
# MAGIC ## Step 3: Monitor Endpoint Deployment Status
# MAGIC
# MAGIC Run this cell to check if endpoint is READY

# COMMAND ----------
# Check endpoint status
endpoint_url = f"https://{workspace_url}/api/2.0/serving-endpoints/{ENDPOINT_NAME}"

response = requests.get(
    endpoint_url,
    headers={"Authorization": f"Bearer {token}"}
)

if response.status_code == 200:
    endpoint_info = response.json()
    status = endpoint_info.get("state", {}).get("ready", "UNKNOWN")
    config_update = endpoint_info.get("state", {}).get("config_update", "UNKNOWN")

    print(f"Endpoint Status:")
    print(f"  Name: {ENDPOINT_NAME}")
    print(f"  Ready: {status}")
    print(f"  Config Update: {config_update}")

    if status == "READY":
        print(f"\n‚úÖ Endpoint is READY! Proceed to Step 4 to test it.")
    else:
        print(f"\n‚è≥ Endpoint not ready yet. Wait a few minutes and re-run this cell.")
        print(f"\nTip: You can also check status in Databricks UI:")
        print(f"  Navigate to: Serving ‚Üí {ENDPOINT_NAME}")
else:
    print(f"‚ùå Error fetching endpoint status: {response.status_code}")
    print(f"Response: {response.text}")

# COMMAND ----------
# MAGIC %md
# MAGIC ## Step 4: Test Serving Endpoint with Sample Prediction
# MAGIC
# MAGIC **Run this ONLY after endpoint is READY (Step 3 shows green status)**

# COMMAND ----------
# Load a test image from Bronze layer
sample_df = spark.sql("""
    SELECT * FROM healthcare_catalog_dev.bronze.kaggle_xray_metadata
    WHERE category = 'NORMAL' LIMIT 1
""")

test_row = sample_df.first()
test_image_path = test_row.file_path
true_category = test_row.category

print(f"Test Image:")
print(f"  Path: {test_image_path}")
print(f"  True Category: {true_category}")

# Load and preprocess image (same as training)
# Unity Catalog volumes use /Volumes/ path directly (no /dbfs prefix)
local_path = test_image_path.replace("dbfs:", "")
img = Image.open(local_path)
img = img.convert('RGB')
img = img.resize((64, 64))
img_array = np.array(img) / 255.0

print(f"  Image shape: {img_array.shape}")

# COMMAND ----------
# Call serving endpoint
invocation_url = f"https://{workspace_url}/serving-endpoints/{ENDPOINT_NAME}/invocations"

payload = {
    "dataframe_records": [{
        "image": img_array.tolist()
    }]
}

print(f"Calling endpoint: {invocation_url}")
print(f"Payload size: {len(json.dumps(payload))} bytes")

try:
    response = requests.post(
        invocation_url,
        headers={"Authorization": f"Bearer {token}"},
        json=payload,
        timeout=30
    )

    if response.status_code == 200:
        result = response.json()
        print(f"\n‚úÖ SUCCESS! Endpoint responded.")
        print(f"\nResponse:")
        print(json.dumps(result, indent=2))

        # Parse prediction
        prediction = result.get('predictions', [[]])[0]
        pred_prob = prediction[0] if isinstance(prediction, list) else prediction

        pred_label = 1 if pred_prob > 0.5 else 0
        pred_label_name = "NORMAL" if pred_label == 0 else "PNEUMONIA"
        confidence = max(pred_prob, 1 - pred_prob)

        print(f"\n" + "="*80)
        print(f"PREDICTION RESULT")
        print(f"="*80)
        print(f"  True Label:     {true_category}")
        print(f"  Predicted:      {pred_label_name}")
        print(f"  Confidence:     {confidence:.2%}")
        print(f"  Prob(PNEUMONIA): {pred_prob:.3f}")
        print(f"  Match:          {'‚úÖ Correct' if pred_label_name == true_category else '‚ùå Incorrect'}")
        print(f"\nüéâ Model Serving endpoint is working!")

    else:
        print(f"\n‚ùå Error: {response.status_code}")
        print(f"Response: {response.text}")

except Exception as e:
    print(f"\n‚ùå Error calling endpoint: {e}")
    print(f"\nTroubleshooting:")
    print(f"  1. Check endpoint status in Step 3 (must be READY)")
    print(f"  2. Verify endpoint name: {ENDPOINT_NAME}")
    print(f"  3. Check Databricks UI ‚Üí Serving ‚Üí {ENDPOINT_NAME}")

# COMMAND ----------
# MAGIC %md
# MAGIC ## Step 5: Batch Test Multiple Images (Optional)
# MAGIC
# MAGIC Test the endpoint with multiple images to verify stability

# COMMAND ----------
# Load 5 test images
test_df = spark.sql("""
    (SELECT * FROM healthcare_catalog_dev.bronze.kaggle_xray_metadata
     WHERE category = 'NORMAL' LIMIT 3)
    UNION ALL
    (SELECT * FROM healthcare_catalog_dev.bronze.kaggle_xray_metadata
     WHERE category = 'PNEUMONIA' LIMIT 2)
""")

test_rows = test_df.collect()

print(f"Testing {len(test_rows)} images...")
print(f"\n" + "="*80)

correct_predictions = 0

for i, row in enumerate(test_rows):
    # Load image
    # Unity Catalog volumes use /Volumes/ path directly (no /dbfs prefix)
    local_path = row.file_path.replace("dbfs:", "")
    img = Image.open(local_path)
    img = img.convert('RGB')
    img = img.resize((64, 64))
    img_array = np.array(img) / 255.0

    # Call endpoint
    payload = {"dataframe_records": [{"image": img_array.tolist()}]}

    try:
        response = requests.post(
            invocation_url,
            headers={"Authorization": f"Bearer {token}"},
            json=payload,
            timeout=30
        )

        if response.status_code == 200:
            result = response.json()
            prediction = result.get('predictions', [[]])[0]
            pred_prob = prediction[0] if isinstance(prediction, list) else prediction

            pred_label_name = "NORMAL" if pred_prob < 0.5 else "PNEUMONIA"
            is_correct = pred_label_name == row.category

            if is_correct:
                correct_predictions += 1

            status_icon = "‚úÖ" if is_correct else "‚ùå"
            print(f"{status_icon} Image {i+1}: True={row.category:<10} Pred={pred_label_name:<10} Conf={max(pred_prob, 1-pred_prob):.2%}")
        else:
            print(f"‚ùå Image {i+1}: Error {response.status_code}")

    except Exception as e:
        print(f"‚ùå Image {i+1}: Exception {e}")

print(f"="*80)
print(f"\nBatch Test Results:")
print(f"  Total: {len(test_rows)} images")
print(f"  Correct: {correct_predictions}/{len(test_rows)} ({correct_predictions/len(test_rows):.1%})")
print(f"\n‚úÖ Serving endpoint is stable and responding correctly!")

# COMMAND ----------
# MAGIC %md
# MAGIC ## Summary: Serving Endpoint Deployed! ‚úÖ
# MAGIC
# MAGIC ### What We Accomplished:
# MAGIC 1. ‚úÖ Deployed MLflow model as REST API serving endpoint
# MAGIC 2. ‚úÖ Endpoint is READY and accepting requests
# MAGIC 3. ‚úÖ Tested with sample predictions
# MAGIC 4. ‚úÖ Verified stability with batch test
# MAGIC
# MAGIC ### Endpoint Details:
# MAGIC - **Name**: `pneumonia-poc-classifier`
# MAGIC - **URL**: `https://{workspace_url}/serving-endpoints/pneumonia-poc-classifier/invocations`
# MAGIC - **Model**: `pneumonia_poc_classifier` version 1
# MAGIC - **Compute**: Serverless (Small, auto-scales to zero)
# MAGIC
# MAGIC ### How to Use:
# MAGIC ```python
# MAGIC import requests
# MAGIC
# MAGIC response = requests.post(
# MAGIC     "https://{workspace_url}/serving-endpoints/pneumonia-poc-classifier/invocations",
# MAGIC     headers={"Authorization": f"Bearer {token}"},
# MAGIC     json={"dataframe_records": [{"image": image_array.tolist()}]}
# MAGIC )
# MAGIC
# MAGIC prediction = response.json()['predictions'][0]
# MAGIC ```
# MAGIC
# MAGIC ### Next Steps (After POC):
# MAGIC 1. Add BI dashboard (TBD - design Gold table integration)
# MAGIC 2. Scale to full 1000 images dataset
# MAGIC 3. Improve model with EfficientNetB0
# MAGIC 4. Hyperparameter tuning via MLflow
# MAGIC
# MAGIC **POC Complete! End-to-end ML pipeline working. üöÄ**
